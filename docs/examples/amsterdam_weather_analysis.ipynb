{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amsterdam Weather Analysis with Conditional Mixture Models\n",
    "\n",
    "This notebook analyzes hourly weather data from Amsterdam using conditional Gaussian mixture models to understand the relationships between temperature, wind speed, and solar radiation over time.\n",
    "\n",
    "**Key Questions:**\n",
    "- How do weather variables relate to seasonal and daily cycles?\n",
    "- What are the typical weather patterns at different times of year?\n",
    "- How well can we model the joint distribution of weather variables?\n",
    "\n",
    "**Data:** Hourly weather data from Amsterdam (2014-2023) including temperature, wind speed, and solar radiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from cgmm import ConditionalGMMRegressor, MixtureOfExpertsRegressor, DiscriminativeConditionalGMMRegressor\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the Amsterdam weather data and prepare it for analysis. We'll focus on three key variables:\n",
    "- **Temperature** (`temp_c`): Air temperature in Celsius\n",
    "- **Wind Speed** (`wind_ms`): Wind speed in m/s (log-transformed)\n",
    "- **Solar Radiation** (`ghi_wm2`): Global horizontal irradiance in W/mÂ² (log-transformed)\n",
    "\n",
    "The conditioning variables will be derived from the datetime to capture seasonal and daily patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (52608, 7)\n",
      "Date range: 2019-01-01 00:00:00 to 2024-12-31 23:00:00\n",
      "\n",
      "Missing values:\n",
      "temp_c     0\n",
      "wind_ms    0\n",
      "ghi_wm2    0\n",
      "dtype: int64\n",
      "\n",
      "Clean dataset shape: (52608, 4)\n",
      "\n",
      "Basic statistics:\n",
      "             temp_c       wind_ms       ghi_wm2\n",
      "count  52608.000000  52608.000000  52608.000000\n",
      "mean      11.327055     15.325150    128.068602\n",
      "std        6.339285      7.743829    198.913009\n",
      "min      -10.300000      0.000000      0.000000\n",
      "25%        6.700000      9.400000      0.000000\n",
      "50%       10.900000     14.100000      5.000000\n",
      "75%       15.900000     19.700000    195.000000\n",
      "max       38.000000     61.400000    869.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the weather data\n",
    "df = pd.read_csv('data/amsterdam_hourly.csv')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df[['temp_c', 'wind_ms', 'ghi_wm2']].isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_clean = df[['datetime', 'temp_c', 'wind_ms', 'ghi_wm2']].dropna()\n",
    "print(f\"\\nClean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_clean[['temp_c', 'wind_ms', 'ghi_wm2']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created cyclical features:\n",
      "- Annual cycle: annual_sin, annual_cos\n",
      "- Daily cycle: daily_sin, daily_cos\n"
     ]
    }
   ],
   "source": [
    "# Create time-based features for conditioning\n",
    "df_clean['year'] = df_clean['datetime'].dt.year\n",
    "df_clean['month'] = df_clean['datetime'].dt.month\n",
    "df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear\n",
    "df_clean['hour'] = df_clean['datetime'].dt.hour\n",
    "\n",
    "# Create cyclical features\n",
    "# Annual cycle (seasonal)\n",
    "df_clean['annual_sin'] = np.sin(2 * np.pi * df_clean['day_of_year'] / 365.25)\n",
    "df_clean['annual_cos'] = np.cos(2 * np.pi * df_clean['day_of_year'] / 365.25)\n",
    "\n",
    "# Daily cycle (diurnal)\n",
    "df_clean['daily_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['daily_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "print(\"Created cyclical features:\")\n",
    "print(\"- Annual cycle: annual_sin, annual_cos\")\n",
    "print(\"- Daily cycle: daily_sin, daily_cos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variables: ['temp_c', 'wind_ms_log', 'ghi_wm2_log']\n",
      "Conditioning variables: ['annual_sin', 'annual_cos', 'daily_sin', 'daily_cos']\n",
      "X shape: (52608, 4), y shape: (52608, 3)\n",
      "\n",
      "Transformation comparison:\n",
      "Wind speed - Original vs Log-transformed:\n",
      "  Original: mean=15.33, std=7.74\n",
      "  Log(1+x): mean=2.68, std=0.50\n",
      "\n",
      "Solar radiation - Original vs Log-transformed:\n",
      "  Original: mean=128.07, std=198.91\n",
      "  Log(1+x): mean=2.55, std=2.63\n"
     ]
    }
   ],
   "source": [
    "# Transform positive variables using log(1+x) to handle zeros\n",
    "df_clean['wind_ms_log'] = np.log1p(df_clean['wind_ms'])  # log(1 + wind_ms)\n",
    "df_clean['ghi_wm2_log'] = np.log1p(df_clean['ghi_wm2'])  # log(1 + ghi_wm2)\n",
    "\n",
    "# Prepare target variables (weather variables)\n",
    "targets = ['temp_c', 'wind_ms_log', 'ghi_wm2_log']\n",
    "y = df_clean[targets].values\n",
    "\n",
    "# Prepare conditioning variables (time-based features)\n",
    "conditioning_vars = ['annual_sin', 'annual_cos', 'daily_sin', 'daily_cos']\n",
    "X = df_clean[conditioning_vars].values\n",
    "\n",
    "print(f\"Target variables: {targets}\")\n",
    "print(f\"Conditioning variables: {conditioning_vars}\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Show transformation effect\n",
    "print(\"\\nTransformation comparison:\")\n",
    "print(\"Wind speed - Original vs Log-transformed:\")\n",
    "print(f\"  Original: mean={df_clean['wind_ms'].mean():.2f}, std={df_clean['wind_ms'].std():.2f}\")\n",
    "print(f\"  Log(1+x): mean={df_clean['wind_ms_log'].mean():.2f}, std={df_clean['wind_ms_log'].std():.2f}\")\n",
    "print(\"\\nSolar radiation - Original vs Log-transformed:\")\n",
    "print(f\"  Original: mean={df_clean['ghi_wm2'].mean():.2f}, std={df_clean['ghi_wm2'].std():.2f}\")\n",
    "print(f\"  Log(1+x): mean={df_clean['ghi_wm2_log'].mean():.2f}, std={df_clean['ghi_wm2_log'].std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative Model Investigation\n",
    "\n",
    "Systematic investigation of the Discriminative Conditional GMM Regressor on the FULL dataset with various hyperparameter settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 21 hyperparameter combinations on FULL dataset\n",
      "Dataset size: X=(52608, 4), y=(52608, 3)\n",
      "\n",
      "Hyperparameter combinations:\n",
      "   1. n_components= 3, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "   2. n_components= 5, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "   3. n_components=10, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "   4. n_components=15, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "   5. n_components= 3, max_iter=200, tol=1e-04, weight_step=0.500\n",
      "   6. n_components= 3, max_iter=300, tol=1e-05, weight_step=0.100\n",
      "   7. n_components= 3, max_iter=500, tol=1e-06, weight_step=0.050\n",
      "   8. n_components= 5, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "   9. n_components= 5, max_iter=200, tol=1e-04, weight_step=0.500\n",
      "  10. n_components= 5, max_iter=300, tol=1e-05, weight_step=0.100\n",
      "  11. n_components= 5, max_iter=500, tol=1e-06, weight_step=0.050\n",
      "  12. n_components= 8, max_iter=100, tol=1e-03, weight_step=1.000\n",
      "  13. n_components= 8, max_iter=200, tol=1e-04, weight_step=0.500\n",
      "  14. n_components= 8, max_iter=300, tol=1e-05, weight_step=0.100\n",
      "  15. n_components= 8, max_iter=500, tol=1e-06, weight_step=0.050\n",
      "  16. n_components=10, max_iter=200, tol=1e-04, weight_step=5.000\n",
      "  17. n_components=10, max_iter=300, tol=1e-05, weight_step=1.000\n",
      "  18. n_components=10, max_iter=500, tol=1e-06, weight_step=0.500\n",
      "  19. n_components=15, max_iter=200, tol=1e-04, weight_step=5.000\n",
      "  20. n_components=15, max_iter=300, tol=1e-05, weight_step=1.000\n",
      "  21. n_components=15, max_iter=500, tol=1e-06, weight_step=0.500\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for systematic testing\n",
    "hyperparameter_grid = [\n",
    "    # (n_components, max_iter, tol, weight_step)\n",
    "    (3, 100, 1e-3, 1),\n",
    "    (5, 100, 1e-3, 1),\n",
    "    (10, 100, 1e-3, 1),\n",
    "    (15, 100, 1e-3, 1),\n",
    "\n",
    "    (3, 200, 1e-4, 0.5),\n",
    "    (3, 300, 1e-5, 0.1),\n",
    "    (3, 500, 1e-6, 0.05),\n",
    "    (5, 100, 1e-3, 1),\n",
    "    (5, 200, 1e-4, 0.5),\n",
    "    (5, 300, 1e-5, 0.1),\n",
    "    (5, 500, 1e-6, 0.05),\n",
    "    (8, 100, 1e-3, 1),\n",
    "    (8, 200, 1e-4, 0.5),\n",
    "    (8, 300, 1e-5, 0.1),\n",
    "    (8, 500, 1e-6, 0.05),\n",
    "    (10, 200, 1e-4, 5),\n",
    "    (10, 300, 1e-5, 1),\n",
    "    (10, 500, 1e-6, 0.5),\n",
    "    (15, 200, 1e-4, 5),\n",
    "    (15, 300, 1e-5, 1),\n",
    "    (15, 500, 1e-6, 0.5)\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(hyperparameter_grid)} hyperparameter combinations on FULL dataset\")\n",
    "print(f\"Dataset size: X={X.shape}, y={y.shape}\")\n",
    "print(\"\\nHyperparameter combinations:\")\n",
    "for i, (n_comp, max_iter, tol, weight_step) in enumerate(hyperparameter_grid):\n",
    "    print(f\"  {i+1:2d}. n_components={n_comp:2d}, max_iter={max_iter:3d}, tol={tol:.0e}, weight_step={weight_step:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DISCRIMINATIVE MODEL HYPERPARAMETER TESTING ===\n",
      "Testing on FULL dataset with consistent evaluation\n",
      "\n",
      "Progress:\n",
      "\n",
      " 1/21: n_comp= 3, max_iter=100, tol=1e-03, weight_step=1.000\n"
     ]
    }
   ],
   "source": [
    "# Test all hyperparameter combinations on FULL dataset\n",
    "results = []\n",
    "\n",
    "print(\"\\n=== DISCRIMINATIVE MODEL HYPERPARAMETER TESTING ===\")\n",
    "print(\"Testing on FULL dataset with consistent evaluation\")\n",
    "print(\"\\nProgress:\")\n",
    "\n",
    "for i, (n_components, max_iter, tol, weight_step) in enumerate(hyperparameter_grid):\n",
    "    print(f\"\\n{i+1:2d}/{len(hyperparameter_grid)}: n_comp={n_components:2d}, max_iter={max_iter:3d}, tol={tol:.0e}, weight_step={weight_step:.3f}\")\n",
    "\n",
    "    # Create model with current hyperparameters\n",
    "    model = DiscriminativeConditionalGMMRegressor(\n",
    "        n_components=n_components,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        weight_step=weight_step,\n",
    "        random_state=42\n",
    "    )   \n",
    "    \n",
    "    # Train on FULL dataset\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Evaluate on FULL dataset (consistent evaluation)\n",
    "    y_pred = model.predict(X)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    log_likelihood = model.score(X, y)  # This is the key metric!\n",
    "    \n",
    "    # Get convergence info\n",
    "    iterations = getattr(model, 'n_iter_', 'Unknown')\n",
    "    converged = getattr(model, 'converged_', 'Unknown')\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'n_components': n_components,\n",
    "        'max_iter': max_iter,\n",
    "        'tol': tol,\n",
    "        'weight_step': weight_step,\n",
    "        'iterations': iterations,\n",
    "        'converged': converged,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'log_likelihood': log_likelihood\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"    Iterations: {iterations}, Converged: {converged}\")\n",
    "    print(f\"    MSE: {mse:.4f}, RÂ²: {r2:.4f}, Log-likelihood: {log_likelihood:.4f}\")\n",
    "    \n",
    "    # Check for issues\n",
    "    if np.isnan(log_likelihood) or np.isinf(log_likelihood):\n",
    "        print(f\"    WARNING: Invalid log-likelihood!\")\n",
    "    if iterations == max_iter:\n",
    "        print(f\"    WARNING: Hit max_iter limit!\")\n",
    "\n",
    "\n",
    "print(\"\\n=== TESTING COMPLETED ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== DISCRIMINATIVE MODEL RESULTS ANALYSIS ===\")\n",
    "print(f\"Successful runs: {len(results_df.dropna())}/{len(results_df)}\")\n",
    "\n",
    "# Sort by log-likelihood (best first)\n",
    "valid_results = results_df.dropna().copy()\n",
    "valid_results = valid_results.sort_values('log_likelihood', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 configurations by log-likelihood:\")\n",
    "print(valid_results[['n_components', 'max_iter', 'tol', 'weight_step', 'iterations', 'converged', 'log_likelihood', 'r2']].head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nWorst 5 configurations by log-likelihood:\")\n",
    "print(valid_results[['n_components', 'max_iter', 'tol', 'weight_step', 'iterations', 'converged', 'log_likelihood', 'r2']].tail(5).to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nLog-likelihood statistics:\")\n",
    "print(f\"  Mean: {valid_results['log_likelihood'].mean():.4f}\")\n",
    "print(f\"  Std:  {valid_results['log_likelihood'].std():.4f}\")\n",
    "print(f\"  Min:  {valid_results['log_likelihood'].min():.4f}\")\n",
    "print(f\"  Max:  {valid_results['log_likelihood'].max():.4f}\")\n",
    "\n",
    "print(f\"\\nIteration statistics:\")\n",
    "print(f\"  Mean: {valid_results['iterations'].mean():.1f}\")\n",
    "print(f\"  Std:  {valid_results['iterations'].std():.1f}\")\n",
    "print(f\"  Min:  {valid_results['iterations'].min()}\")\n",
    "print(f\"  Max:  {valid_results['iterations'].max()}\")\n",
    "\n",
    "print(f\"\\nConvergence rate: {(valid_results['converged'] == True).mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Log-likelihood vs n_components\n",
    "axes[0, 0].scatter(valid_results['n_components'], valid_results['log_likelihood'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Number of Components')\n",
    "axes[0, 0].set_ylabel('Log-likelihood')\n",
    "axes[0, 0].set_title('Log-likelihood vs Components')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood vs iterations\n",
    "axes[0, 1].scatter(valid_results['iterations'], valid_results['log_likelihood'], alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Iterations')\n",
    "axes[0, 1].set_ylabel('Log-likelihood')\n",
    "axes[0, 1].set_title('Log-likelihood vs Iterations')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood vs tolerance\n",
    "axes[0, 2].scatter(valid_results['tol'], valid_results['log_likelihood'], alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Tolerance')\n",
    "axes[0, 2].set_ylabel('Log-likelihood')\n",
    "axes[0, 2].set_title('Log-likelihood vs Tolerance')\n",
    "axes[0, 2].set_xscale('log')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood vs weight_step\n",
    "axes[1, 0].scatter(valid_results['weight_step'], valid_results['log_likelihood'], alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Weight Step')\n",
    "axes[1, 0].set_ylabel('Log-likelihood')\n",
    "axes[1, 0].set_title('Log-likelihood vs Weight Step')\n",
    "axes[1, 0].set_xscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# RÂ² vs log-likelihood\n",
    "axes[1, 1].scatter(valid_results['log_likelihood'], valid_results['r2'], alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Log-likelihood')\n",
    "axes[1, 1].set_ylabel('RÂ²')\n",
    "axes[1, 1].set_title('RÂ² vs Log-likelihood')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Iterations vs tolerance\n",
    "axes[1, 2].scatter(valid_results['tol'], valid_results['iterations'], alpha=0.7)\n",
    "axes[1, 2].set_xlabel('Tolerance')\n",
    "axes[1, 2].set_ylabel('Iterations')\n",
    "axes[1, 2].set_title('Iterations vs Tolerance')\n",
    "axes[1, 2].set_xscale('log')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model and analyze it in detail\n",
    "best_config = valid_results.iloc[0]\n",
    "print(f\"\\n=== BEST MODEL ANALYSIS ===\")\n",
    "print(f\"Best configuration:\")\n",
    "print(f\"  n_components: {best_config['n_components']}\")\n",
    "print(f\"  max_iter: {best_config['max_iter']}\")\n",
    "print(f\"  tol: {best_config['tol']}\")\n",
    "print(f\"  weight_step: {best_config['weight_step']}\")\n",
    "print(f\"  Performance: log_likelihood={best_config['log_likelihood']:.4f}, r2={best_config['r2']:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = DiscriminativeConditionalGMMRegressor(\n",
    "    n_components=int(best_config['n_components']),\n",
    "    max_iter=int(best_config['max_iter']),\n",
    "    tol=best_config['tol'],\n",
    "    weight_step=best_config['weight_step'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining best model on FULL dataset...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Final evaluation\n",
    "y_pred_best = best_model.predict(X)\n",
    "mse_best = mean_squared_error(y, y_pred_best)\n",
    "r2_best = r2_score(y, y_pred_best)\n",
    "log_likelihood_best = best_model.score(X, y)\n",
    "\n",
    "print(f\"\\nFinal performance on FULL dataset:\")\n",
    "print(f\"  MSE: {mse_best:.4f}\")\n",
    "print(f\"  RÂ²: {r2_best:.4f}\")\n",
    "print(f\"  Log-likelihood: {log_likelihood_best:.4f}\")\n",
    "print(f\"  Iterations: {getattr(best_model, 'n_iter_', 'Unknown')}\")\n",
    "print(f\"  Converged: {getattr(best_model, 'converged_', 'Unknown')}\")\n",
    "\n",
    "# Check prediction ranges\n",
    "print(f\"\\nPrediction analysis:\")\n",
    "print(f\"  Target range: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "print(f\"  Prediction range: [{y_pred_best.min():.3f}, {y_pred_best.max():.3f}]\")\n",
    "print(f\"  Target mean: {y.mean():.3f}\")\n",
    "print(f\"  Prediction mean: {y_pred_best.mean():.3f}\")\n",
    "\n",
    "# Check for any issues\n",
    "nan_pred = np.isnan(y_pred_best).sum()\n",
    "inf_pred = np.isinf(y_pred_best).sum()\n",
    "if nan_pred > 0:\n",
    "    print(f\"  WARNING: {nan_pred} NaN predictions!\")\n",
    "if inf_pred > 0:\n",
    "    print(f\"  WARNING: {inf_pred} infinite predictions!\")\n",
    "\n",
    "print(f\"\\n=== BEST MODEL ANALYSIS COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison: Fixed Discriminative vs Conditional GMM\n",
    "print(\"\\n=== FINAL PERFORMANCE COMPARISON ===\")\n",
    "\n",
    "# Use a larger subset for meaningful comparison\n",
    "X_final = X[:1000]\n",
    "y_final = y[:1000]\n",
    "\n",
    "print(f\"Final comparison on subset: X={X_final.shape}, y={y_final.shape}\")\n",
    "\n",
    "# Train Conditional GMM\n",
    "conditional_final = ConditionalGMMRegressor(\n",
    "    n_components=3,\n",
    "    random_state=42\n",
    ")\n",
    "conditional_final.fit(X_final, y_final)\n",
    "\n",
    "# Train Discriminative (using corrected source code)\n",
    "discriminative_final = DiscriminativeConditionalGMMRegressor(\n",
    "    n_components=3,\n",
    "    max_iter=100,\n",
    "    tol=1e-4,\n",
    "    weight_step=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "discriminative_final.fit(X_final, y_final)\n",
    "\n",
    "# Compare log-likelihoods\n",
    "conditional_ll_final = conditional_final.score(X_final, y_final)\n",
    "discriminative_ll_final = discriminative_final.score(X_final, y_final)\n",
    "\n",
    "print(f\"\\nLog-likelihood comparison:\")\n",
    "print(f\"  Conditional GMM: {conditional_ll_final:.6f}\")\n",
    "print(f\"  Discriminative: {discriminative_ll_final:.6f}\")\n",
    "print(f\"  Difference (Conditional - Discriminative): {conditional_ll_final - discriminative_ll_final:.6f}\")\n",
    "\n",
    "# Compare RÂ² scores\n",
    "conditional_pred_final = conditional_final.predict(X_final)\n",
    "discriminative_pred_final = discriminative_final.predict(X_final)\n",
    "\n",
    "conditional_r2_final = r2_score(y_final, conditional_pred_final)\n",
    "discriminative_r2_final = r2_score(y_final, discriminative_pred_final)\n",
    "\n",
    "print(f\"\\nRÂ² comparison:\")\n",
    "print(f\"  Conditional GMM: {conditional_r2_final:.6f}\")\n",
    "print(f\"  Discriminative: {discriminative_r2_final:.6f}\")\n",
    "print(f\"  Difference (Conditional - Discriminative): {conditional_r2_final - discriminative_r2_final:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED: Test the properly fixed Discriminative model\n",
    "print(\"\\n=== TESTING CORRECTED LOG_PROB IMPLEMENTATION ===\")\n",
    "\n",
    "# Test on a small subset\n",
    "X_test_corrected = X[:100]\n",
    "y_test_corrected = y[:100]\n",
    "\n",
    "print(f\"Testing corrected implementation on subset: X={X_test_corrected.shape}, y={y_test_corrected.shape}\")\n",
    "\n",
    "# Create the CORRECTED Discriminative model\n",
    "corrected_discriminative = DiscriminativeConditionalGMMRegressor(\n",
    "    n_components=3,\n",
    "    max_iter=50,\n",
    "    tol=1e-4,\n",
    "    weight_step=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "corrected_discriminative.fit(X_test_corrected, y_test_corrected)\n",
    "\n",
    "# Test consistency between training and evaluation\n",
    "internal_ll = corrected_discriminative._mean_conditional_loglik(X_test_corrected, y_test_corrected)\n",
    "score_ll = corrected_discriminative.score(X_test_corrected, y_test_corrected)\n",
    "log_prob_ll = corrected_discriminative.log_prob(X_test_corrected, y_test_corrected)\n",
    "log_prob_mean = np.mean(log_prob_ll)\n",
    "\n",
    "print(f\"\\nCorrected implementation results:\")\n",
    "print(f\"  Internal log-likelihood (training): {internal_ll:.6f}\")\n",
    "print(f\"  Score method log-likelihood: {score_ll:.6f}\")\n",
    "print(f\"  Log_prob method mean: {log_prob_mean:.6f}\")\n",
    "print(f\"  Difference (internal - score): {internal_ll - score_ll:.6f}\")\n",
    "print(f\"  Difference (internal - log_prob): {internal_ll - log_prob_mean:.6f}\")\n",
    "\n",
    "if abs(internal_ll - score_ll) < 1e-6:\n",
    "    print(\"âœ… SUCCESS: Corrected implementation - log-likelihood calculations are now consistent!\")\n",
    "    print(\"The Discriminative model now correctly computes log p(y|x) for evaluation.\")\n",
    "else:\n",
    "    print(\"âŒ Still inconsistent after correction.\")\n",
    "\n",
    "print(f\"\\n=== KEY INSIGHT ===\")\n",
    "print(\"The Discriminative model's log_prob() should return log p(y|x), not the discriminative EM objective.\")\n",
    "print(\"The discriminative EM objective (log p(x,y) - log p(x)) is only used during training.\")\n",
    "print(\"For evaluation, we want log p(y|x) which is what the base class correctly computes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amsterdam Weather Analysis with Conditional Mixture Models\n",
    "\n",
    "This notebook analyzes hourly weather data from Amsterdam using conditional Gaussian mixture models to understand the relationships between temperature, wind speed, and solar radiation over time.\n",
    "\n",
    "**Key Questions:**\n",
    "- How do weather variables relate to seasonal and daily cycles?\n",
    "- What are the typical weather patterns at different times of year?\n",
    "- How well can we model the joint distribution of weather variables?\n",
    "\n",
    "**Data:** Hourly weather data from Amsterdam (2014-2023) including temperature, wind speed, and solar radiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from cgmm import ConditionalGMMRegressor, MixtureOfExpertsRegressor, DiscriminativeConditionalGMMRegressor\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Load the Amsterdam weather data and prepare it for analysis. We'll focus on three key variables:\n",
    "- **Temperature** (`temp_c`): Air temperature in Celsius\n",
    "- **Wind Speed** (`wind_ms`): Wind speed in m/s (log-transformed)\n",
    "- **Solar Radiation** (`ghi_wm2`): Global horizontal irradiance in W/mÂ² (log-transformed)\n",
    "\n",
    "The conditioning variables will be derived from the datetime to capture seasonal and daily patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the weather data\n",
    "df = pd.read_csv('data/amsterdam_hourly.csv')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['datetime'].min()} to {df['datetime'].max()}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df[['temp_c', 'wind_ms', 'ghi_wm2']].isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_clean = df[['datetime', 'temp_c', 'wind_ms', 'ghi_wm2']].dropna()\n",
    "print(f\"\\nClean dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df_clean[['temp_c', 'wind_ms', 'ghi_wm2']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Let's investigate why all hyperparameters give identical results\n",
    "print(\"\\n=== DEBUGGING IDENTICAL RESULTS ===\")\n",
    "\n",
    "# Test just 3 different hyperparameter combinations\n",
    "debug_combinations = [\n",
    "    (3, 50, 1e-3, 0.1),   # Low iterations, high tolerance, high weight_step\n",
    "    (3, 200, 1e-6, 0.01), # High iterations, low tolerance, low weight_step  \n",
    "    (5, 100, 1e-4, 0.05)  # Different n_components\n",
    "]\n",
    "\n",
    "print(\"Testing 3 different hyperparameter combinations:\")\n",
    "for i, (n_components, max_iter, tol, weight_step) in enumerate(debug_combinations):\n",
    "    print(f\"\\n{i+1}. n_comp={n_components}, max_iter={max_iter}, tol={tol:.0e}, weight_step={weight_step:.3f}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = DiscriminativeConditionalGMMRegressor(\n",
    "        n_components=n_components,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        weight_step=weight_step,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on small subset for debugging\n",
    "    X_debug = X[:100]\n",
    "    y_debug = y[:100]\n",
    "    model.fit(X_debug, y_debug)\n",
    "    \n",
    "    # Check if parameters actually changed\n",
    "    print(f\"  Final weights: {model.weights_}\")\n",
    "    print(f\"  Final means shape: {model.means_.shape}\")\n",
    "    print(f\"  Final means[0]: {model.means_[0]}\")\n",
    "    print(f\"  Iterations: {model.n_iter_}, Converged: {model.converged_}\")\n",
    "    \n",
    "    # Check log-likelihood\n",
    "    ll = model.score(X_debug, y_debug)\n",
    "    print(f\"  Log-likelihood: {ll:.6f}\")\n",
    "    \n",
    "    # Check if the model is actually different\n",
    "    if i > 0:\n",
    "        prev_weights = debug_combinations[i-1][0]\n",
    "        if prev_weights == n_components:\n",
    "            print(f\"  Same n_components as previous - weights should be different\")\n",
    "        else:\n",
    "            print(f\"  Different n_components - should have different shape\")\n",
    "\n",
    "print(f\"\\n=== DIAGNOSIS ===\")\n",
    "print(\"If all results are identical, the issue could be:\")\n",
    "print(\"1. The discriminative EM algorithm is not actually updating parameters\")\n",
    "print(\"2. The convergence criteria is too strict and stopping immediately\")\n",
    "print(\"3. The random_state is making all models identical\")\n",
    "print(\"4. The log_prob method has a bug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL DEBUG: Let's check if the models are actually different\n",
    "print(\"\\n=== CRITICAL DEBUG: CHECKING IF MODELS ARE ACTUALLY DIFFERENT ===\")\n",
    "\n",
    "# Test the exact same hyperparameters from your output\n",
    "test_combinations = [\n",
    "    (3, 100, 1e-3, 0.1),   # Should give 5 iterations\n",
    "    (3, 200, 1e-4, 0.05),  # Should give 6 iterations\n",
    "]\n",
    "\n",
    "for i, (n_components, max_iter, tol, weight_step) in enumerate(test_combinations):\n",
    "    print(f\"\\n=== TESTING COMBINATION {i+1} ===\")\n",
    "    print(f\"n_comp={n_components}, max_iter={max_iter}, tol={tol:.0e}, weight_step={weight_step:.3f}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = DiscriminativeConditionalGMMRegressor(\n",
    "        n_components=n_components,\n",
    "        max_iter=max_iter,\n",
    "        tol=tol,\n",
    "        weight_step=weight_step,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train on small subset\n",
    "    X_test = X[:100]\n",
    "    y_test = y[:100]\n",
    "    model.fit(X_test, y_test)\n",
    "    \n",
    "    # Check if parameters are actually different\n",
    "    print(f\"Final weights: {model.weights_}\")\n",
    "    print(f\"Final means[0]: {model.means_[0]}\")\n",
    "    print(f\"Final covariances[0] shape: {model.covariances_[0].shape}\")\n",
    "    print(f\"Final covariances[0] diagonal: {np.diag(model.covariances_[0])}\")\n",
    "    \n",
    "    # Check predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Prediction range: [{y_pred.min():.6f}, {y_pred.max():.6f}]\")\n",
    "    print(f\"Prediction mean: {y_pred.mean():.6f}\")\n",
    "    print(f\"Prediction std: {y_pred.std():.6f}\")\n",
    "    \n",
    "    # Check log-likelihood\n",
    "    ll = model.score(X_test, y_test)\n",
    "    print(f\"Log-likelihood: {ll:.6f}\")\n",
    "    \n",
    "    # Store for comparison\n",
    "    if i == 0:\n",
    "        prev_weights = model.weights_.copy()\n",
    "        prev_means = model.means_.copy()\n",
    "        prev_covs = model.covariances_.copy()\n",
    "        prev_pred = y_pred.copy()\n",
    "        prev_ll = ll\n",
    "    else:\n",
    "        # Compare with previous\n",
    "        weights_diff = np.abs(model.weights_ - prev_weights).max()\n",
    "        means_diff = np.abs(model.means_ - prev_means).max()\n",
    "        covs_diff = np.abs(model.covariances_ - prev_covs).max()\n",
    "        pred_diff = np.abs(y_pred - prev_pred).max()\n",
    "        ll_diff = abs(ll - prev_ll)\n",
    "        \n",
    "        print(f\"\\n=== COMPARISON WITH PREVIOUS ===\")\n",
    "        print(f\"Weights difference: {weights_diff:.2e}\")\n",
    "        print(f\"Means difference: {means_diff:.2e}\")\n",
    "        print(f\"Covariances difference: {covs_diff:.2e}\")\n",
    "        print(f\"Predictions difference: {pred_diff:.2e}\")\n",
    "        print(f\"Log-likelihood difference: {ll_diff:.2e}\")\n",
    "        \n",
    "        if weights_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Weights are identical!\")\n",
    "        if means_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Means are identical!\")\n",
    "        if covs_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Covariances are identical!\")\n",
    "        if pred_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Predictions are identical!\")\n",
    "        if ll_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Log-likelihood is identical!\")\n",
    "\n",
    "print(f\"\\n=== DIAGNOSIS ===\")\n",
    "print(\"If all differences are near zero, the discriminative EM is not working!\")\n",
    "print(\"The algorithm might be converging immediately without updating parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test if random_state is causing identical results\n",
    "print(\"\\n=== DEBUG: TESTING RANDOM STATE EFFECT ===\")\n",
    "\n",
    "# Test with different random states\n",
    "random_states = [42, 123, 456]\n",
    "\n",
    "for i, random_state in enumerate(random_states):\n",
    "    print(f\"\\n--- Random State {random_state} ---\")\n",
    "    \n",
    "    model = DiscriminativeConditionalGMMRegressor(\n",
    "        n_components=3,\n",
    "        max_iter=50,\n",
    "        tol=1e-4,\n",
    "        weight_step=0.01,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_test = X[:100]\n",
    "    y_test = y[:100]\n",
    "    model.fit(X_test, y_test)\n",
    "    \n",
    "    print(f\"Final weights: {model.weights_}\")\n",
    "    print(f\"Final means[0]: {model.means_[0]}\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    ll = model.score(X_test, y_test)\n",
    "    print(f\"Log-likelihood: {ll:.6f}\")\n",
    "    \n",
    "    if i == 0:\n",
    "        prev_weights = model.weights_.copy()\n",
    "        prev_means = model.means_.copy()\n",
    "        prev_ll = ll\n",
    "    else:\n",
    "        weights_diff = np.abs(model.weights_ - prev_weights).max()\n",
    "        means_diff = np.abs(model.means_ - prev_means).max()\n",
    "        ll_diff = abs(ll - prev_ll)\n",
    "        \n",
    "        print(f\"Difference from previous: weights={weights_diff:.2e}, means={means_diff:.2e}, ll={ll_diff:.2e}\")\n",
    "        \n",
    "        if weights_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Different random states give identical weights!\")\n",
    "        if means_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Different random states give identical means!\")\n",
    "        if ll_diff < 1e-10:\n",
    "            print(\"ðŸš¨ BUG: Different random states give identical log-likelihood!\")\n",
    "\n",
    "print(f\"\\n=== HYPOTHESIS ===\")\n",
    "print(\"If different random states give identical results, the discriminative EM is broken!\")\n",
    "print(\"The algorithm might not be using the random state properly or not updating parameters at all.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Let's create a custom version that shows what's happening during training\n",
    "print(\"\\n=== DEBUGGING DISCRIMINATIVE EM ALGORITHM ===\")\n",
    "\n",
    "class DebugDiscriminativeConditionalGMMRegressor(DiscriminativeConditionalGMMRegressor):\n",
    "    \"\"\"Debug version that shows what's happening during training.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Call parent fit but with debugging\n",
    "        print(f\"  Starting fit with n_components={self.n_components}, max_iter={self.max_iter}, tol={self.tol}, weight_step={self.weight_step}\")\n",
    "        \n",
    "        # Validate and shape data\n",
    "        X, y = validate_data(\n",
    "            self,\n",
    "            X,\n",
    "            y,\n",
    "            accept_sparse=False,\n",
    "            y_numeric=True,\n",
    "            multi_output=True,\n",
    "        )\n",
    "        y = np.asarray(y, dtype=float)\n",
    "        if y.ndim == 1:\n",
    "            y = y[:, None]\n",
    "\n",
    "        n, Dx = X.shape\n",
    "        Dy = y.shape[1]\n",
    "        D = Dx + Dy\n",
    "        self.n_features_in_ = Dx\n",
    "        self.n_targets_ = Dy\n",
    "\n",
    "        # Init by joint GMM on Z=[X|Y]\n",
    "        Z = np.concatenate([X, y], axis=1)\n",
    "        gmm = GaussianMixture(\n",
    "            n_components=self.n_components,\n",
    "            covariance_type=self.covariance_type,\n",
    "            tol=self.tol,\n",
    "            reg_covar=max(self.reg_covar, 1e-6),\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state,\n",
    "            init_params=self.init_params,\n",
    "        ).fit(Z)\n",
    "\n",
    "        # Store joint params\n",
    "        self.weights_ = gmm.weights_.astype(float, copy=True)\n",
    "        self.means_ = gmm.means_.astype(float, copy=True)\n",
    "        self.covariances_ = gmm.covariances_.astype(float, copy=True)\n",
    "\n",
    "        print(f\"  Initial weights: {self.weights_}\")\n",
    "        print(f\"  Initial means[0]: {self.means_[0]}\")\n",
    "\n",
    "        # Ensure PD / positivity\n",
    "        if self.covariance_type == \"full\":\n",
    "            for k in range(self.n_components):\n",
    "                self.covariances_[k] = self._ensure_positive_definite(self.covariances_[k])\n",
    "        else:\n",
    "            self.covariances_ = np.maximum(self.covariances_, self.reg_covar)\n",
    "\n",
    "        # Initialize logits for weights\n",
    "        self._alpha_ = np.log(self.weights_ + np.finfo(float).tiny)\n",
    "\n",
    "        # Run discriminative EM with debugging\n",
    "        prev_ll = -np.inf\n",
    "        self.converged_ = False\n",
    "        for it in range(1, self.max_iter + 1):\n",
    "            prev_means = self.means_.copy()\n",
    "            prev_covs = self.covariances_.copy()\n",
    "            prev_weights = self.weights_.copy()\n",
    "\n",
    "            r, s = self._e_step(X, y)\n",
    "            self._m_step_joint(X, y, r)\n",
    "            self._m_step_weights(r, s)\n",
    "\n",
    "            # Check parameter changes\n",
    "            means_delta = float(np.abs(self.means_ - prev_means).max())\n",
    "            covs_delta = float(np.abs(self.covariances_ - prev_covs).max())\n",
    "            weights_delta = float(np.abs(self.weights_ - prev_weights).max())\n",
    "            param_delta = max(means_delta, covs_delta, weights_delta)\n",
    "            \n",
    "            if it <= 5 or it % 10 == 0:  # Show first 5 iterations and every 10th\n",
    "                print(f\"    Iter {it}: means_delta={means_delta:.2e}, covs_delta={covs_delta:.2e}, weights_delta={weights_delta:.2e}\")\n",
    "                print(f\"      Weights: {self.weights_}\")\n",
    "            \n",
    "            if param_delta < self.tol:\n",
    "                self.converged_ = True\n",
    "                print(f\"    Converged at iteration {it} with param_delta={param_delta:.2e}\")\n",
    "                break\n",
    "\n",
    "            ll = self._mean_conditional_loglik(X, y)\n",
    "            prev_ll = ll\n",
    "\n",
    "        self.lower_bound_ = float(self._mean_conditional_loglik(X, y))\n",
    "        self.n_iter_ = int(it)\n",
    "        print(f\"  Final weights: {self.weights_}\")\n",
    "        print(f\"  Final means[0]: {self.means_[0]}\")\n",
    "        print(f\"  Final iterations: {self.n_iter_}, Converged: {self.converged_}\")\n",
    "        return self\n",
    "\n",
    "# Test the debug version\n",
    "print(\"Testing debug version:\")\n",
    "debug_model = DebugDiscriminativeConditionalGMMRegressor(\n",
    "    n_components=3,\n",
    "    max_iter=20,\n",
    "    tol=1e-4,\n",
    "    weight_step=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_debug = X[:50]  # Very small subset\n",
    "y_debug = y[:50]\n",
    "debug_model.fit(X_debug, y_debug)\n",
    "\n",
    "ll_debug = debug_model.score(X_debug, y_debug)\n",
    "print(f\"Final log-likelihood: {ll_debug:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create time-based features for conditioning\n",
    "df_clean['year'] = df_clean['datetime'].dt.year\n",
    "df_clean['month'] = df_clean['datetime'].dt.month\n",
    "df_clean['day_of_year'] = df_clean['datetime'].dt.dayofyear\n",
    "df_clean['hour'] = df_clean['datetime'].dt.hour\n",
    "\n",
    "# Create cyclical features\n",
    "# Annual cycle (seasonal)\n",
    "df_clean['annual_sin'] = np.sin(2 * np.pi * df_clean['day_of_year'] / 365.25)\n",
    "df_clean['annual_cos'] = np.cos(2 * np.pi * df_clean['day_of_year'] / 365.25)\n",
    "\n",
    "# Daily cycle (diurnal)\n",
    "df_clean['daily_sin'] = np.sin(2 * np.pi * df_clean['hour'] / 24)\n",
    "df_clean['daily_cos'] = np.cos(2 * np.pi * df_clean['hour'] / 24)\n",
    "\n",
    "print(\"Created cyclical features:\")\n",
    "print(\"- Annual cycle: annual_sin, annual_cos\")\n",
    "print(\"- Daily cycle: daily_sin, daily_cos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Transform positive variables using log(1+x) to handle zeros\n",
    "df_clean['wind_ms_log'] = np.log1p(df_clean['wind_ms'])  # log(1 + wind_ms)\n",
    "df_clean['ghi_wm2_log'] = np.log1p(df_clean['ghi_wm2'])  # log(1 + ghi_wm2)\n",
    "\n",
    "# Prepare target variables (weather variables)\n",
    "targets = ['temp_c', 'wind_ms_log', 'ghi_wm2_log']\n",
    "y = df_clean[targets].values\n",
    "\n",
    "# Prepare conditioning variables (time-based features)\n",
    "conditioning_vars = ['annual_sin', 'annual_cos', 'daily_sin', 'daily_cos']\n",
    "X = df_clean[conditioning_vars].values\n",
    "\n",
    "print(f\"Target variables: {targets}\")\n",
    "print(f\"Conditioning variables: {conditioning_vars}\")\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Show transformation effect\n",
    "print(\"\\nTransformation comparison:\")\n",
    "print(\"Wind speed - Original vs Log-transformed:\")\n",
    "print(f\"  Original: mean={df_clean['wind_ms'].mean():.2f}, std={df_clean['wind_ms'].std():.2f}\")\n",
    "print(f\"  Log(1+x): mean={df_clean['wind_ms_log'].mean():.2f}, std={df_clean['wind_ms_log'].std():.2f}\")\n",
    "print(\"\\nSolar radiation - Original vs Log-transformed:\")\n",
    "print(f\"  Original: mean={df_clean['ghi_wm2'].mean():.2f}, std={df_clean['ghi_wm2'].std():.2f}\")\n",
    "print(f\"  Log(1+x): mean={df_clean['ghi_wm2_log'].mean():.2f}, std={df_clean['ghi_wm2_log'].std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Let's explore the weather patterns and relationships before building our models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Select samples to plot\n",
    "df_sample = df_clean.sample(n=10000, random_state=42).sort_index()\n",
    "\n",
    "\n",
    "# Create 3x3 grid: rows = variables, columns = time perspectives\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "\n",
    "# Define colors for each variable\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green\n",
    "variables = ['temp_c', 'wind_ms_log', 'ghi_wm2_log']\n",
    "var_names = ['Temperature', 'Wind Speed (log)', 'Solar Radiation (log)']\n",
    "units = ['Â°C', 'log(m/s)', 'log(W/mÂ²)']\n",
    "\n",
    "# Row 1: Temperature\n",
    "# Hourly over time\n",
    "axes[0, 0].plot(df_sample['datetime'], df_sample['temp_c'], alpha=0.7, linewidth=0.8, color=colors[0])\n",
    "axes[0, 0].set_title('Temperature Over Time')\n",
    "axes[0, 0].set_ylabel('Temperature (Â°C)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# vs Day of Year\n",
    "axes[0, 1].scatter(df_sample['day_of_year'], df_sample['temp_c'], alpha=0.6, s=1, color=colors[0])\n",
    "axes[0, 1].set_title('Temperature vs Day of Year')\n",
    "axes[0, 1].set_xlabel('Day of Year')\n",
    "axes[0, 1].set_ylabel('Temperature (Â°C)')\n",
    "\n",
    "# vs Hour of Day\n",
    "axes[0, 2].scatter(df_sample['hour'], df_sample['temp_c'], alpha=0.6, s=1, color=colors[0])\n",
    "axes[0, 2].set_title('Temperature vs Hour of Day')\n",
    "axes[0, 2].set_xlabel('Hour of Day')\n",
    "axes[0, 2].set_ylabel('Temperature (Â°C)')\n",
    "axes[0, 2].set_xticks(range(0, 24, 4))\n",
    "\n",
    "# Row 2: Wind Speed\n",
    "# Hourly over time\n",
    "axes[1, 0].plot(df_sample['datetime'], df_sample['wind_ms_log'], alpha=0.7, linewidth=0.8, color=colors[1])\n",
    "axes[1, 0].set_title('Wind Speed Over Time')\n",
    "axes[1, 0].set_ylabel('Wind Speed (log)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# vs Day of Year\n",
    "axes[1, 1].scatter(df_sample['day_of_year'], df_sample['wind_ms_log'], alpha=0.6, s=1, color=colors[1])\n",
    "axes[1, 1].set_title('Wind Speed vs Day of Year')\n",
    "axes[1, 1].set_xlabel('Day of Year')\n",
    "axes[1, 1].set_ylabel('Wind Speed (log)')\n",
    "\n",
    "# vs Hour of Day\n",
    "axes[1, 2].scatter(df_sample['hour'], df_sample['wind_ms_log'], alpha=0.6, s=1, color=colors[1])\n",
    "axes[1, 2].set_title('Wind Speed vs Hour of Day')\n",
    "axes[1, 2].set_xlabel('Hour of Day')\n",
    "axes[1, 2].set_ylabel('Wind Speed (log)')\n",
    "axes[1, 2].set_xticks(range(0, 24, 4))\n",
    "\n",
    "# Row 3: Solar Radiation\n",
    "# Hourly over time\n",
    "axes[2, 0].plot(df_sample['datetime'], df_sample['ghi_wm2_log'], alpha=0.7, linewidth=0.8, color=colors[2])\n",
    "axes[2, 0].set_title('Solar Radiation Over Time')\n",
    "axes[2, 0].set_ylabel('Solar Radiation (log)')\n",
    "axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# vs Day of Year\n",
    "axes[2, 1].scatter(df_sample['day_of_year'], df_sample['ghi_wm2_log'], alpha=0.6, s=1, color=colors[2])\n",
    "axes[2, 1].set_title('Solar Radiation vs Day of Year')\n",
    "axes[2, 1].set_xlabel('Day of Year')\n",
    "axes[2, 1].set_ylabel('Solar Radiation (log)')\n",
    "\n",
    "# vs Hour of Day\n",
    "axes[2, 2].scatter(df_sample['hour'], df_sample['ghi_wm2_log'], alpha=0.6, s=1, color=colors[2])\n",
    "axes[2, 2].set_title('Solar Radiation vs Hour of Day')\n",
    "axes[2, 2].set_xlabel('Hour of Day')\n",
    "axes[2, 2].set_ylabel('Solar Radiation (log)')\n",
    "axes[2, 2].set_xticks(range(0, 24, 4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some statistics about solar radiation\n",
    "print(\"Solar Radiation Statistics:\")\n",
    "print(f\"Original values - Min: {df_clean['ghi_wm2'].min():.1f}, Max: {df_clean['ghi_wm2'].max():.1f}, Mean: {df_clean['ghi_wm2'].mean():.1f}\")\n",
    "print(f\"Log-transformed - Min: {df_clean['ghi_wm2_log'].min():.3f}, Max: {df_clean['ghi_wm2_log'].max():.3f}, Mean: {df_clean['ghi_wm2_log'].mean():.3f}\")\n",
    "print(f\"Percentage of zero values: {(df_clean['ghi_wm2'] == 0).mean()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 8))\n",
    "\n",
    "# Correlation matrix of weather variables\n",
    "weather_vars = ['temp_c', 'wind_ms_log', 'ghi_wm2_log']\n",
    "corr_matrix = df_sample[weather_vars].corr()\n",
    "\n",
    "# Create heatmap using matplotlib\n",
    "im = axes[0].imshow(corr_matrix.values, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[0].set_xticks(range(len(corr_matrix.columns)))\n",
    "axes[0].set_yticks(range(len(corr_matrix.index)))\n",
    "axes[0].set_xticklabels(corr_matrix.columns, rotation=45)\n",
    "axes[0].set_yticklabels(corr_matrix.index)\n",
    "axes[0].set_title('Weather Variables Correlation')\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(corr_matrix.index)):\n",
    "    for j in range(len(corr_matrix.columns)):\n",
    "        text = axes[0].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "\n",
    "# Scatter plot matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(df_sample[weather_vars], alpha=0.6, ax=axes[1], diagonal='hist', hist_kwds={'bins':30})\n",
    "axes[1].set_title('Weather Variables Scatter Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation insights:\")\n",
    "print(f\"Temperature vs Wind Speed: {corr_matrix.loc['temp_c', 'wind_ms_log']:.3f}\")\n",
    "print(f\"Temperature vs Solar Radiation: {corr_matrix.loc['temp_c', 'ghi_wm2_log']:.3f}\")\n",
    "print(f\"Wind Speed vs Solar Radiation: {corr_matrix.loc['wind_ms_log', 'ghi_wm2_log']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now let's train different conditional mixture models to understand how weather variables depend on time patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Use full dataset for training and analysis\n",
    "X_train, y_train = X, y\n",
    "\n",
    "print(f\"Full dataset: {X_train.shape[0]} samples\")\n",
    "print(f\"Training period: {df_clean.iloc[0]['datetime']} to {df_clean.iloc[-1]['datetime']}\")\n",
    "print(\"Using full dataset for both training and visualization to avoid artifacts from small test sets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Train different models with better hyperparameters\n",
    "n_components = 5\n",
    "\n",
    "# Try different configurations to fix the issues\n",
    "models = {\n",
    "    'ConditionalGMM': ConditionalGMMRegressor(n_components=n_components, random_state=42, max_iter=200),\n",
    "    'MixtureOfExperts': MixtureOfExpertsRegressor(\n",
    "        n_components=n_components, \n",
    "        random_state=42, \n",
    "        max_iter=200,\n",
    "        gating_max_iter=100,\n",
    "        gating_tol=1e-4\n",
    "    ),\n",
    "    'Discriminative': DiscriminativeConditionalGMMRegressor(\n",
    "        n_components=n_components, \n",
    "        random_state=42,\n",
    "        max_iter=200,\n",
    "        tol=1e-8,  # Much stricter tolerance to force more iterations\n",
    "        weight_step=0.01  # Smaller step size for more stable convergence\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Training models...\")\n",
    "failed_models = []\n",
    "for name, model in models.items():\n",
    "    print(f\"  Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Check if model converged\n",
    "        if hasattr(model, 'converged_'):\n",
    "            print(f\"    Converged: {model.converged_}\")\n",
    "        if hasattr(model, 'n_iter_'):\n",
    "            print(f\"    Iterations: {model.n_iter_}\")\n",
    "        \n",
    "        # Evaluate performance on full dataset\n",
    "        y_pred = model.predict(X_train)\n",
    "        mse = mean_squared_error(y_train, y_pred)\n",
    "        r2 = r2_score(y_train, y_pred)\n",
    "        log_likelihood = model.score(X_train, y_train)\n",
    "        \n",
    "        # Check for NaN or infinite values\n",
    "        nan_pred = np.isnan(y_pred).sum()\n",
    "        inf_pred = np.isinf(y_pred).sum()\n",
    "        nan_ll = np.isnan(log_likelihood) or np.isinf(log_likelihood)\n",
    "        \n",
    "        print(f\"    MSE: {mse:.4f}, RÂ²: {r2:.4f}, Log-likelihood: {log_likelihood:.4f}\")\n",
    "        if nan_pred > 0:\n",
    "            print(f\"    WARNING: {nan_pred} NaN predictions!\")\n",
    "        if inf_pred > 0:\n",
    "            print(f\"    WARNING: {inf_pred} infinite predictions!\")\n",
    "        if nan_ll:\n",
    "            print(f\"    WARNING: Invalid log-likelihood!\")\n",
    "            \n",
    "        # Check prediction ranges\n",
    "        print(f\"    Prediction range: [{y_pred.min():.3f}, {y_pred.max():.3f}]\")\n",
    "        print(f\"    Target range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")\n",
    "        # Mark for removal\n",
    "        failed_models.append(name)\n",
    "\n",
    "# Remove failed models after iteration\n",
    "for name in failed_models:\n",
    "    del models[name]\n",
    "\n",
    "print(f\"\\nTraining completed! {len(models)} models successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic analysis of model behavior\n",
    "print(\"=== DIAGNOSTIC ANALYSIS ===\")\n",
    "print(f\"Data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"y range: [{y_train.min():.3f}, {y_train.max():.3f}]\")\n",
    "print(f\"X std: {X_train.std():.3f}\")\n",
    "print(f\"y std: {y_train.std():.3f}\")\n",
    "\n",
    "# Check for any issues with the data\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"X has NaN: {np.isnan(X_train).any()}\")\n",
    "print(f\"y has NaN: {np.isnan(y_train).any()}\")\n",
    "print(f\"X has Inf: {np.isinf(X_train).any()}\")\n",
    "print(f\"y has Inf: {np.isinf(y_train).any()}\")\n",
    "\n",
    "# Test each model with a small sample\n",
    "print(f\"\\nTesting models with small sample:\")\n",
    "test_idx = np.random.choice(len(X_train), 100, replace=False)\n",
    "X_test_small = X_train[test_idx]\n",
    "y_test_small = y_train[test_idx]\n",
    "\n",
    "# Create a list of model names to avoid dictionary modification during iteration\n",
    "model_names = list(models.keys())\n",
    "\n",
    "for name in model_names:\n",
    "    if name not in models:  # Skip if model was removed\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{name}:\")\n",
    "    try:\n",
    "        model = models[name]\n",
    "        \n",
    "        # Test prediction\n",
    "        y_pred_small = model.predict(X_test_small)\n",
    "        print(f\"  Prediction shape: {y_pred_small.shape}\")\n",
    "        print(f\"  Prediction range: [{y_pred_small.min():.3f}, {y_pred_small.max():.3f}]\")\n",
    "        \n",
    "        # Test sampling\n",
    "        samples = model.sample(X_test_small[:5], n_samples=3)\n",
    "        print(f\"  Sample shape: {samples.shape}\")\n",
    "        print(f\"  Sample range: [{samples.min():.3f}, {samples.max():.3f}]\")\n",
    "        \n",
    "        # Test log_prob\n",
    "        log_probs = model.log_prob(X_test_small[:5], y_test_small[:5])\n",
    "        print(f\"  Log prob shape: {log_probs.shape}\")\n",
    "        print(f\"  Log prob range: [{log_probs.min():.3f}, {log_probs.max():.3f}]\")\n",
    "        \n",
    "        # Test log-likelihood (score method)\n",
    "        log_likelihood = model.score(X_test_small, y_test_small)\n",
    "        print(f\"  Log-likelihood: {log_likelihood:.4f}\")\n",
    "        \n",
    "        # Check for NaN or infinite values in log-likelihood\n",
    "        if np.isnan(log_likelihood) or np.isinf(log_likelihood):\n",
    "            print(f\"  WARNING: Invalid log-likelihood!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(\"\\n=== END DIAGNOSTIC ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific investigation of Discriminative model issues\n",
    "print(\"=== DISCRIMINATIVE MODEL INVESTIGATION ===\")\n",
    "\n",
    "if 'Discriminative' in models:\n",
    "    disc_model = models['Discriminative']\n",
    "    \n",
    "    # Check model parameters\n",
    "    print(\"Model parameters:\")\n",
    "    print(f\"  n_components: {disc_model.n_components}\")\n",
    "    print(f\"  covariance_type: {disc_model.covariance_type}\")\n",
    "    print(f\"  max_iter: {disc_model.max_iter}\")\n",
    "    print(f\"  tol: {disc_model.tol}\")\n",
    "    \n",
    "    # Check fitted parameters\n",
    "    if hasattr(disc_model, 'weights_'):\n",
    "        print(f\"  Weights shape: {disc_model.weights_.shape}\")\n",
    "        print(f\"  Weights range: [{disc_model.weights_.min():.6f}, {disc_model.weights_.max():.6f}]\")\n",
    "        print(f\"  Weights sum per component: {disc_model.weights_.sum(axis=0)}\")\n",
    "    \n",
    "    if hasattr(disc_model, 'means_'):\n",
    "        print(f\"  Means shape: {disc_model.means_.shape}\")\n",
    "        print(f\"  Means range: [{disc_model.means_.min():.3f}, {disc_model.means_.max():.3f}]\")\n",
    "    \n",
    "    if hasattr(disc_model, 'covariances_'):\n",
    "        print(f\"  Covariances shape: {disc_model.covariances_.shape}\")\n",
    "        print(f\"  Covariances range: [{disc_model.covariances_.min():.6f}, {disc_model.covariances_.max():.6f}]\")\n",
    "    \n",
    "    # Test with different parameters\n",
    "    print(\"\\nTesting with different parameters:\")\n",
    "    test_params = [\n",
    "        {'n_components': 3, 'max_iter': 50, 'tol': 1e-3},\n",
    "        {'n_components': 2, 'max_iter': 100, 'tol': 1e-4},\n",
    "        {'n_components': 4, 'max_iter': 200, 'tol': 1e-5}\n",
    "    ]\n",
    "    \n",
    "    for i, params in enumerate(test_params):\n",
    "        print(f\"\\nTest {i+1}: {params}\")\n",
    "        try:\n",
    "            test_model = DiscriminativeConditionalGMMRegressor(**params, random_state=42)\n",
    "            test_model.fit(X_train[:1000], y_train[:1000])  # Use smaller sample for speed\n",
    "            \n",
    "            y_pred_test = test_model.predict(X_train[:100])\n",
    "            mse_test = mean_squared_error(y_train[:100], y_pred_test)\n",
    "            r2_test = r2_score(y_train[:100], y_pred_test)\n",
    "            \n",
    "            print(f\"  MSE: {mse_test:.4f}, RÂ²: {r2_test:.4f}\")\n",
    "            print(f\"  Converged: {getattr(test_model, 'converged_', 'Unknown')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(\"\\n=== END DISCRIMINATIVE INVESTIGATION ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into Discriminative model convergence issues\n",
    "print(\"=== DISCRIMINATIVE CONVERGENCE INVESTIGATION ===\")\n",
    "\n",
    "# Test with a very simple case first\n",
    "print(\"Testing with simple 2D case (temperature only):\")\n",
    "X_simple = X_train[:, :2]  # Only annual and daily cycles\n",
    "y_simple = y_train[:, 0:1]  # Only temperature\n",
    "\n",
    "print(f\"Simple data shape: X={X_simple.shape}, y={y_simple.shape}\")\n",
    "\n",
    "# Test different configurations with much stricter tolerances\n",
    "test_configs = [\n",
    "    {'n_components': 3, 'max_iter': 100, 'tol': 1e-8, 'weight_step': 0.01},\n",
    "    {'n_components': 2, 'max_iter': 200, 'tol': 1e-9, 'weight_step': 0.005},\n",
    "    {'n_components': 4, 'max_iter': 300, 'tol': 1e-10, 'weight_step': 0.02},\n",
    "    {'n_components': 3, 'max_iter': 150, 'tol': 1e-7, 'weight_step': 0.01}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(test_configs):\n",
    "    print(f\"\\nConfig {i+1}: {config}\")\n",
    "    try:\n",
    "        test_model = DiscriminativeConditionalGMMRegressor(**config, random_state=42)\n",
    "        test_model.fit(X_simple[:1000], y_simple[:1000])  # Use smaller sample\n",
    "        \n",
    "        print(f\"  Converged: {getattr(test_model, 'converged_', 'Unknown')}\")\n",
    "        print(f\"  Iterations: {getattr(test_model, 'n_iter_', 'Unknown')}\")\n",
    "        \n",
    "        if hasattr(test_model, 'weights_'):\n",
    "            print(f\"  Weights: {test_model.weights_[:3]}\")  # Show first 3 weights\n",
    "        \n",
    "        # Test prediction\n",
    "        y_pred_test = test_model.predict(X_simple[:10])\n",
    "        print(f\"  Prediction range: [{y_pred_test.min():.3f}, {y_pred_test.max():.3f}]\")\n",
    "        \n",
    "        # Test log-likelihood\n",
    "        log_likelihood = test_model.score(X_simple[:100], y_simple[:100])\n",
    "        print(f\"  Log-likelihood: {log_likelihood:.4f}\")\n",
    "        \n",
    "        # Check for invalid log-likelihood\n",
    "        if np.isnan(log_likelihood) or np.isinf(log_likelihood):\n",
    "            print(f\"  WARNING: Invalid log-likelihood!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(\"\\n=== END CONVERGENCE INVESTIGATION ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate MoE temperature prediction issues\n",
    "print(\"=== MOE TEMPERATURE INVESTIGATION ===\")\n",
    "\n",
    "if 'MixtureOfExperts' in models:\n",
    "    moe_model = models['MixtureOfExperts']\n",
    "    \n",
    "    print(\"MoE Model Analysis:\")\n",
    "    print(f\"  n_components: {moe_model.n_components}\")\n",
    "    print(f\"  max_iter: {moe_model.max_iter}\")\n",
    "    print(f\"  converged: {getattr(moe_model, 'converged_', 'Unknown')}\")\n",
    "    print(f\"  iterations: {getattr(moe_model, 'n_iter_', 'Unknown')}\")\n",
    "    \n",
    "    # Check the internal parameters\n",
    "    if hasattr(moe_model, '_params'):\n",
    "        params = moe_model._params\n",
    "        print(f\"  Expert A shape: {params.A.shape if params.A is not None else 'None'}\")\n",
    "        if params.A is not None:\n",
    "            print(f\"  Expert A range: [{params.A.min():.3f}, {params.A.max():.3f}]\")\n",
    "        print(f\"  Expert b shape: {params.b.shape}\")\n",
    "        print(f\"  Expert b range: [{params.b.min():.3f}, {params.b.max():.3f}]\")\n",
    "        print(f\"  Gating W shape: {params.W.shape}\")\n",
    "        print(f\"  Gating W range: [{params.W.min():.3f}, {params.W.max():.3f}]\")\n",
    "        print(f\"  Gating c shape: {params.c.shape}\")\n",
    "        print(f\"  Gating c range: [{params.c.min():.3f}, {params.c.max():.3f}]\")\n",
    "    \n",
    "    # Test predictions on different time points\n",
    "    print(\"\\nTesting predictions on different time points:\")\n",
    "    test_times = [\n",
    "        (\"Winter midnight\", 355, 0),    # Winter midnight\n",
    "        (\"Summer noon\", 172, 12),       # Summer noon\n",
    "        (\"Spring dawn\", 80, 6),         # Spring dawn\n",
    "    ]\n",
    "    \n",
    "    for name, day_of_year, hour in test_times:\n",
    "        X_test_time = np.array([[\n",
    "            np.sin(2 * np.pi * day_of_year / 365.25),\n",
    "            np.cos(2 * np.pi * day_of_year / 365.25),\n",
    "            np.sin(2 * np.pi * hour / 24),\n",
    "            np.cos(2 * np.pi * hour / 24)\n",
    "        ]])\n",
    "        \n",
    "        y_pred_time = moe_model.predict(X_test_time)\n",
    "        print(f\"  {name}: {y_pred_time[0]}\")\n",
    "        \n",
    "        # Check gating probabilities\n",
    "        if hasattr(moe_model, '_compute_conditional_mixture'):\n",
    "            mixture_params = moe_model._compute_conditional_mixture(X_test_time)\n",
    "            weights = mixture_params['weights'][0]\n",
    "            print(f\"    Gating weights: {weights[:5]}\")  # Show first 5 weights\n",
    "\n",
    "print(\"\\n=== END MOE INVESTIGATION ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Systematic convergence testing\n",
    "print(\"=== SYSTEMATIC CONVERGENCE TESTING ===\")\n",
    "\n",
    "# Test with very strict tolerances to force more iterations\n",
    "tolerance_tests = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\n",
    "\n",
    "print(\"Testing different tolerance values:\")\n",
    "for tol in tolerance_tests:\n",
    "    print(f\"\\nTolerance: {tol}\")\n",
    "    try:\n",
    "        test_model = DiscriminativeConditionalGMMRegressor(\n",
    "            n_components=5,\n",
    "            random_state=42,\n",
    "            max_iter=500,  # High max_iter to see actual convergence\n",
    "            tol=tol,\n",
    "            weight_step=0.01\n",
    "        )\n",
    "        test_model.fit(X_train[:2000], y_train[:2000])  # Use subset for speed\n",
    "        \n",
    "        iterations = getattr(test_model, 'n_iter_', 'Unknown')\n",
    "        converged = getattr(test_model, 'converged_', 'Unknown')\n",
    "        log_likelihood = test_model.score(X_train[:100], y_train[:100])\n",
    "        \n",
    "        print(f\"  Iterations: {iterations}\")\n",
    "        print(f\"  Converged: {converged}\")\n",
    "        print(f\"  Log-likelihood: {log_likelihood:.4f}\")\n",
    "        \n",
    "        # Check if we hit max_iter\n",
    "        if iterations == 500:\n",
    "            print(f\"  WARNING: Hit max_iter limit!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "\n",
    "print(\"\\n=== END CONVERGENCE TESTING ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Visualization\n",
    "\n",
    "Let's analyze how well our models capture the weather patterns and create visualizations of the learned distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    r2 = r2_score(y_train, y_pred)\n",
    "    log_likelihood = model.score(X_train, y_train)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'RÂ²': r2,\n",
    "        'Log-Likelihood': log_likelihood\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualize performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['MSE', 'RÂ²', 'Log-Likelihood']\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(results_df['Model'], results_df[metric], alpha=0.7)\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Analyze seasonal patterns using the best model\n",
    "best_model_name = results_df.loc[results_df['Log-Likelihood'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Create a grid of time points for analysis\n",
    "days_of_year = np.linspace(1, 365, 12)  # 12 points per year\n",
    "hours_of_day = [6, 12, 18, 0]  # Dawn, noon, dusk, midnight\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, hour in enumerate(hours_of_day):\n",
    "    # Create conditioning variables for this time\n",
    "    X_condition = np.zeros((len(days_of_year), 4))\n",
    "    X_condition[:, 0] = np.sin(2 * np.pi * days_of_year / 365.25)  # annual_sin\n",
    "    X_condition[:, 1] = np.cos(2 * np.pi * days_of_year / 365.25)  # annual_cos\n",
    "    X_condition[:, 2] = np.sin(2 * np.pi * hour / 24)  # daily_sin\n",
    "    X_condition[:, 3] = np.cos(2 * np.pi * hour / 24)  # daily_cos\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = best_model.predict(X_condition)\n",
    "    \n",
    "    # Plot temperature and wind speed\n",
    "    ax = axes[i]\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    line1 = ax.plot(days_of_year, y_pred[:, 0], 'b-', linewidth=2, label='Temperature')\n",
    "    line2 = ax2.plot(days_of_year, y_pred[:, 1], 'r-', linewidth=2, label='Wind Speed (log)')\n",
    "    \n",
    "    ax.set_xlabel('Day of Year')\n",
    "    ax.set_ylabel('Temperature (Â°C)', color='b')\n",
    "    ax2.set_ylabel('Wind Speed (log)', color='r')\n",
    "    ax.set_title(f'Hour {hour:02d}:00 - Seasonal Patterns')\n",
    "    \n",
    "    # Add month labels\n",
    "    month_days = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    ax.set_xticks(month_days)\n",
    "    ax.set_xticklabels(month_names)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Analyze joint densities at specific time points\n",
    "time_points = [\n",
    "    ('Winter Solstice (Dec 21, 12:00)', 355, 12),  # Winter noon\n",
    "    ('Spring Equinox (Mar 21, 12:00)', 80, 12),    # Spring noon\n",
    "    ('Summer Solstice (Jun 21, 12:00)', 172, 12),  # Summer noon\n",
    "    ('Autumn Equinox (Sep 21, 12:00)', 264, 12)   # Autumn noon\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (title, day_of_year, hour) in enumerate(time_points):\n",
    "    # Create conditioning variables\n",
    "    X_condition = np.array([[\n",
    "        np.sin(2 * np.pi * day_of_year / 365.25),  # annual_sin\n",
    "        np.cos(2 * np.pi * day_of_year / 365.25),  # annual_cos\n",
    "        np.sin(2 * np.pi * hour / 24),             # daily_sin\n",
    "        np.cos(2 * np.pi * hour / 24)              # daily_cos\n",
    "    ]])\n",
    "    \n",
    "    # Get conditional mixture\n",
    "    gmm = best_model.condition(X_condition)\n",
    "    \n",
    "    # Generate samples from the conditional distribution\n",
    "    n_samples = 1000\n",
    "    samples = gmm.sample(n_samples)[0]  # (n_samples, 3)\n",
    "    \n",
    "    # Create a grid for density calculation\n",
    "    temp_range = np.linspace(samples[:, 0].min() - 2, samples[:, 0].max() + 2, 50)\n",
    "    wind_range = np.linspace(samples[:, 1].min() - 0.5, samples[:, 1].max() + 0.5, 50)\n",
    "    temp_grid, wind_grid = np.meshgrid(temp_range, wind_range)\n",
    "    \n",
    "    # Calculate density for each grid point\n",
    "    grid_points = np.column_stack([temp_grid.ravel(), wind_grid.ravel()])\n",
    "    \n",
    "    # Create dummy solar radiation values for the grid (we'll use the mean)\n",
    "    solar_mean = samples[:, 2].mean()\n",
    "    grid_solar = np.full((grid_points.shape[0],), solar_mean)\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    log_probs = gmm.score_samples(np.column_stack([grid_points, grid_solar.reshape(-1, 1)]))\n",
    "    density = np.exp(log_probs).reshape(temp_grid.shape)\n",
    "    \n",
    "    # Plot temperature vs wind speed\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot density contours\n",
    "    contour = ax.contour(temp_grid, wind_grid, density, levels=8, alpha=0.7, colors='black', linewidths=1)\n",
    "    ax.clabel(contour, inline=True, fontsize=8, fmt='%.3f')\n",
    "    \n",
    "    # Plot samples with solar radiation color\n",
    "    scatter = ax.scatter(samples[:, 0], samples[:, 1], alpha=0.6, s=20, c=samples[:, 2], \n",
    "                        cmap='viridis', label='Solar Radiation (log)')\n",
    "    \n",
    "    ax.set_xlabel('Temperature (Â°C)')\n",
    "    ax.set_ylabel('Wind Speed (log)')\n",
    "    ax.set_title(f'{title}\\nTemperature vs Wind Speed (with PDF contours)')\n",
    "    \n",
    "    # Add colorbar for solar radiation\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Solar Radiation (log)')\n",
    "    \n",
    "    # Add some statistics\n",
    "    temp_mean = samples[:, 0].mean()\n",
    "    wind_mean = samples[:, 1].mean()\n",
    "    ax.text(0.05, 0.95, f'Mean Temp: {temp_mean:.1f}Â°C\\nMean Wind: {wind_mean:.2f}', \n",
    "            transform=ax.transAxes, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Analyze diurnal patterns (daily cycles)\n",
    "hours = np.arange(0, 24)\n",
    "day_of_year = 172  # Summer solstice\n",
    "\n",
    "# Create conditioning variables for each hour\n",
    "X_condition = np.zeros((24, 4))\n",
    "X_condition[:, 0] = np.sin(2 * np.pi * day_of_year / 365.25)  # annual_sin\n",
    "X_condition[:, 1] = np.cos(2 * np.pi * day_of_year / 365.25)  # annual_cos\n",
    "X_condition[:, 2] = np.sin(2 * np.pi * hours / 24)  # daily_sin\n",
    "X_condition[:, 3] = np.cos(2 * np.pi * hours / 24)  # daily_cos\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_condition)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Temperature throughout the day\n",
    "axes[0, 0].plot(hours, y_pred[:, 0], 'b-', linewidth=2, marker='o')\n",
    "axes[0, 0].set_title('Temperature Throughout the Day (Summer Solstice)')\n",
    "axes[0, 0].set_xlabel('Hour of Day')\n",
    "axes[0, 0].set_ylabel('Temperature (Â°C)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Wind speed throughout the day\n",
    "axes[0, 1].plot(hours, y_pred[:, 1], 'r-', linewidth=2, marker='o')\n",
    "axes[0, 1].set_title('Wind Speed Throughout the Day (Summer Solstice)')\n",
    "axes[0, 1].set_xlabel('Hour of Day')\n",
    "axes[0, 1].set_ylabel('Wind Speed (log)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Solar radiation throughout the day\n",
    "axes[1, 0].plot(hours, y_pred[:, 2], 'g-', linewidth=2, marker='o')\n",
    "axes[1, 0].set_title('Solar Radiation Throughout the Day (Summer Solstice)')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Solar Radiation (log)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# All variables together\n",
    "ax = axes[1, 1]\n",
    "ax2 = ax.twinx()\n",
    "ax3 = ax.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))\n",
    "\n",
    "line1 = ax.plot(hours, y_pred[:, 0], 'b-', linewidth=2, label='Temperature')\n",
    "line2 = ax2.plot(hours, y_pred[:, 1], 'r-', linewidth=2, label='Wind Speed')\n",
    "line3 = ax3.plot(hours, y_pred[:, 2], 'g-', linewidth=2, label='Solar Radiation')\n",
    "\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Temperature (Â°C)', color='b')\n",
    "ax2.set_ylabel('Wind Speed (log)', color='r')\n",
    "ax3.set_ylabel('Solar Radiation (log)', color='g')\n",
    "ax.set_title('All Variables Throughout the Day')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Analyze uncertainty in predictions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    # Generate samples for uncertainty analysis (use first 100 samples from full dataset)\n",
    "    n_samples = 1000\n",
    "    samples = model.sample(X_train[:100], n_samples=n_samples)  # (100, n_samples, 3)\n",
    "    \n",
    "    # Calculate prediction intervals\n",
    "    temp_mean = samples[:, :, 0].mean(axis=1)\n",
    "    temp_std = samples[:, :, 0].std(axis=1)\n",
    "    temp_lower = np.percentile(samples[:, :, 0], 2.5, axis=1)\n",
    "    temp_upper = np.percentile(samples[:, :, 0], 97.5, axis=1)\n",
    "    \n",
    "    # Plot temperature predictions with uncertainty\n",
    "    ax = axes[i]\n",
    "    test_indices = range(len(temp_mean))\n",
    "    \n",
    "    ax.fill_between(test_indices, temp_lower, temp_upper, alpha=0.3, label='95% Prediction Interval')\n",
    "    ax.plot(test_indices, temp_mean, 'b-', linewidth=2, label='Mean Prediction')\n",
    "    ax.plot(test_indices, y_train[:100, 0], 'r--', linewidth=1, label='Actual')\n",
    "    \n",
    "    ax.set_title(f'{name} - Temperature Predictions')\n",
    "    ax.set_xlabel('Test Sample Index')\n",
    "    ax.set_ylabel('Temperature (Â°C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgmm-3qL4zxqN-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
